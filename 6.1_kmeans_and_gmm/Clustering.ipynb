{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Your full name:** <br>\n",
        "**Your student number:**"
      ],
      "metadata": {
        "id": "6A5lUL446lZK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7FNlKIA7Ywm"
      },
      "source": [
        "# Clustering Methods from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dacf6liW7Ywr"
      },
      "source": [
        "During this assignment, we will implement the K-means clustering and gaussian mixture model algorithms from scratch using Pytorch.<br>\n",
        "The data used for training the unsupervised models was generated to show the distinction between K-means and Gaussian Mixture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJznzJVH7Yws"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad4lVvAR7Ywu"
      },
      "outputs": [],
      "source": [
        "def data_generator(cluster_nums, cluster_means, cluster_var,\n",
        "                   background_range, background_noise_nums):\n",
        "    \"\"\"Generates data using a mutivariate normal distribution\n",
        "\n",
        "    Args:\n",
        "        cluster_nums: A positive integer for number of clusters.\n",
        "        cluster_means: A list or numpy array describing the mean of each cluster.\n",
        "        cluster_var: A list or numpy array describing the variance of each cluster.\n",
        "        background_range: A tuple containing two float for the range of the background noise.\n",
        "        background_noise_nums: Number of background noise points.\n",
        "\n",
        "    Returns:\n",
        "        A 2d numpy array.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for num, mu, var in zip(cluster_nums, cluster_means, cluster_var):\n",
        "        data += [np.random.multivariate_normal(mu, np.diag(var), num)]\n",
        "    data = np.vstack(data)\n",
        "    noise = np.random.uniform(background_range[0], background_range[1], size=(background_noise_nums, data.shape[-1]))\n",
        "    data = np.append(data, noise, axis=0)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkeMSEL97Ywv"
      },
      "outputs": [],
      "source": [
        "X = data_generator(cluster_nums=[400,600,800],\n",
        "                   cluster_means=[[0.5, 0.5],\n",
        "                                  [6, 1.5],\n",
        "                                  [1, 7]],\n",
        "                   cluster_var=[[1, 3],\n",
        "                                [2, 2],\n",
        "                                [6, 2]],\n",
        "                   background_range=[[-10, -15],\n",
        "                                     [15, 20]],\n",
        "                   background_noise_nums=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A- Using matplotlib library, show generated data in 2D figure**"
      ],
      "metadata": {
        "id": "dMpl9jS00djg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HnuTiaK7Yww"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY3toClo7Ywx"
      },
      "source": [
        "Our goal is to separate the data into groups where the samples in each group are similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoZ7uMQj7Ywy"
      },
      "source": [
        "## Standardize the data\n",
        "The K-means method is a distance based clustering approach.<br>\n",
        "It is mandatory to standardize the data to prevent the features with higher values to contribute more than the others.<br>\n",
        "In a real world application, we would have standardize the data performing the following lines."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B- Using Scikit-learn library, standardize the data and convert them to float with pytorch**"
      ],
      "metadata": {
        "id": "ysRHBtsB09s4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYSxSgyo7Ywy"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgFcDwQY7Ywz"
      },
      "source": [
        "### K-Means Clustering\n",
        "K-means algorithm is an iterative approach that tries to partition a dataset into $K$ predefined clusters where each data point belongs to only one cluster.\n",
        "            \n",
        "This algorithm works that way:\n",
        "- specify number of clusters $K$\n",
        "- randomly initialize one centroid in space for each cluster\n",
        "- for each point, compute the euclidean distance between the point and each centroid and assign the point to the closest centroid\n",
        "- change the centroids values based on the points present in each cluster and repeat the previous step until the centroids do not change anymore\n",
        "\n",
        "The approach, K-means follows to solve this problem is called Expectation-Maximization. The E-step assign each point to a cluster, and the M-step refines the values of the centroid based on the points inside each cluster.\n",
        "\n",
        "More formally, the objective function to minimize is as follows:<br>\n",
        "$$\n",
        "    \\large J = \\sum_{i=1}^{m} \\sum_{k=1}^{K} \\mathbb{I}(z_i = k)||x_i - \\mu_k||_2^2\n",
        "$$<br>\n",
        "where $z_i$ is the cluster assigned to $x_i$ and $\\mu_k$ is the mean of the cluster $k$.\n",
        "\n",
        "The E-step is defined as:<br>\n",
        "$$\n",
        "    \\large z_i^{*} = \\text{argmin}_{k} ||x_i - \\mu_k||_2^2\n",
        "$$<br>\n",
        "\n",
        "And the M-step is defined as:<br>\n",
        "$$\n",
        "    \\large \\mu_k = \\frac{1}{\\sum_{i=1}^m \\mathbb{I}(z_i = k)} \\sum_{i=1}^m  x_i\\mathbb{I}(z_i = k)\n",
        "$$<br>\n",
        "\n",
        "In practice, we should run multiple K-means with different initialization of the centroids and keep the parameters that minimizes the objective function. Since K-means is a distance based algorithm, is it mandatory to standardize the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C- Complete #TODO parts**"
      ],
      "metadata": {
        "id": "3s-KKNDR2I7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnpAzYGn7Yw0"
      },
      "outputs": [],
      "source": [
        "class KMeansClustering():\n",
        "    \"\"\"K-Means class\n",
        "\n",
        "    Attributes:\n",
        "        n_clusters: A integer describing in how many clusters to separate the data.\n",
        "        centroids: A torch tensor containing in each column the mean of the n'th\n",
        "            cluster.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self, n_clusters=5):\n",
        "        \"\"\"Inits KmeansClustering class setting the number of clusters\"\"\"\n",
        "        self.n_clusters = n_clusters\n",
        "        self.centroids = None\n",
        "        \n",
        "    \n",
        "    def fit_transform(self, X, n_iter=20):\n",
        "        \"\"\"Trains the KMeans and clusterize the input data\n",
        "        \n",
        "        Args:\n",
        "            X: A torch tensor for the data to clusterize.\n",
        "            n_iters: A integer describing the number of iterations of\n",
        "                expectation maximization to perform.\n",
        "        \n",
        "        \"\"\"\n",
        "        size = #TODO\n",
        "        # Find min and max values to generate a random centroid in this range\n",
        "        xmax = #TODO\n",
        "        xmin = #TODO\n",
        "        \n",
        "        dists = torch.zeros((size, self.n_clusters))\n",
        "        best_loss = 1e10\n",
        "        pred = None\n",
        "        \n",
        "        for _ in range(n_iter):\n",
        "            centroids = #TODO\n",
        "            old_loss = -1\n",
        "            while 1:\n",
        "                for i in range(self.n_clusters):  # E-step: assign each point to a cluster\n",
        "                    #TODO\n",
        "                \n",
        "                for i in range(self.n_clusters):  # M-step: re-compute the centroids\n",
        "                    #TODO\n",
        "                    \n",
        "                new_loss = #TODO  # Loss: sum distance between points and centroid\n",
        "                if old_loss == new_loss:\n",
        "                    break\n",
        "                old_loss = new_loss\n",
        "            if new_loss < best_loss:\n",
        "                best_loss = new_loss\n",
        "                pred = labels\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D- Declare a K-means model using \"KMeansClustering\" class with 3 clusters and fit it with standard data and finally show clustered data in 2D figure**"
      ],
      "metadata": {
        "id": "E4R-6ji-2Hke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75BFPrW67Yw1"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9ouCnNF7Yw2"
      },
      "source": [
        "\n",
        "Let's see if we can get a better result using a Gaussian Mixture model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yxMUFNo7Yw2"
      },
      "source": [
        "### Gaussian Mixture Model\n",
        "In order to understand how we train a Gaussian Mixture model, we must explain the expectation-maximization algorithm (EM) first.<br>\n",
        "The EM algorithm is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables. EM is an iterative algorithm that starts from an initial estimate of the parameters of a probabilistic model $\\boldsymbol{\\theta}$ and then proceeds to iteratively update $\\boldsymbol{\\theta}$ until convergence. This algorithm consists to iteratively apply an expectation step and a maximization step.\n",
        "            \n",
        "Let's consider the EM algorithm for a multivariate Gaussian Mixture model with $K$ mixture components, observed variables $\\boldsymbol{X}$ and latent variables $\\boldsymbol{Z}$ such as:<br>\n",
        "\n",
        "$$\n",
        "\\large\n",
        "\\begin{aligned}\n",
        "    P(\\boldsymbol{x}_i | \\boldsymbol{\\theta}) &= \\sum_{k=1}^{K} \\pi_k P(\\boldsymbol{x}_i | \\boldsymbol{z}_i, \\boldsymbol{\\theta}_k)\\\\\n",
        "    &= \\sum_{k=1}^{K} P(\\boldsymbol{z}_i = k) \\mathcal{N}(\\boldsymbol{x}_i; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
        "\\end{aligned}\n",
        "$$<br>\n",
        "where each component of the mixture model is the normal distribution.\n",
        "Let's see why the MLE of a gaussian mixture model is hard to compute. The likelihood of a gaussian mixture distribution is:<br>\n",
        "$$\n",
        "\\large\n",
        "    L(\\boldsymbol{\\theta} | \\boldsymbol{X}) = \\prod_{i=1}^N\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\boldsymbol{x}_i; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n",
        "$$<br>\n",
        "\n",
        "This the log-likelihood is as follows:<br>\n",
        "$$\n",
        "\\large\n",
        "    \\mathcal{L}(\\boldsymbol{\\theta} | \\boldsymbol{X}) = \\sum_{i=1}^N \\log\\left(\\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\boldsymbol{x}_i; \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\\right)\n",
        "$$<br>\n",
        "\n",
        "Because of the sum inside the $\\log$, we cannot estimate $\\boldsymbol{\\mu}_k$ and $\\boldsymbol{\\sigma^2}$ without knowing $\\boldsymbol{Z}$. That is why we would use EM in this kind of situation.\n",
        "\n",
        "The EM algorithm has two steps, the expectation step known as E-step, assigns to each data point the probability that they belong to each\n",
        "components of the mixture model. Whereas the maximization step, known as M-step, re-evaluate the parameters of each mixture component based on the estimated values generated in the E-step.\n",
        "\n",
        "More formally, during the E-step, we calculate the likelihood of each data point using the estimated parameters:<br>\n",
        "$$\n",
        "\\large\n",
        "    f(\\boldsymbol{x_i}|\\mu_k, \\boldsymbol{\\Sigma}_k) = \\frac{1}{\\sqrt{(2\\pi)^m|\\boldsymbol{\\Sigma}_k|}} \\exp\\left(-\\frac{(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^\\top \\boldsymbol{\\Sigma}_k^{-1}(\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)}{2}\\right)\n",
        "$$<br>\n",
        "where $m$ is the number of features in the input data.\n",
        "\n",
        "Then we compute the probability that $\\boldsymbol{x}_i$ came from the $k^{th}$ gaussian distribution:<br>\n",
        "$$\n",
        "\\Large\n",
        "    p_{ik} = \\frac{\\pi_k f(\\boldsymbol{x_i}|\\mu_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{j=1}^{K} \\pi_j f(\\boldsymbol{x_i}|\\mu_j, \\boldsymbol{\\Sigma}_j)}\n",
        "$$<br>\n",
        "\n",
        "For the M-step, we update the parameters of the mixture as follows:<br>\n",
        "$$\n",
        "\\Large\n",
        "\\begin{aligned}\n",
        "    \\pi_k &= \\frac{1}{N}\\sum_{i=1}^{N} p_{ik}\\\\\n",
        "    \\boldsymbol{\\mu}_k &= \\frac{1}{\\sum_{i=1}^{N} p_{ik}} \\sum_{i=1}^{N} p_{ik} \\boldsymbol{x_i}\\\\\n",
        "    \\boldsymbol{\\Sigma}_k &= \\frac{1}{\\sum_{i=1}^{N} p_{ik}} \\sum_{i=1}^{N} p_{ik} (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k) (\\boldsymbol{x}_i - \\boldsymbol{\\mu}_k)^\\top\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JlMqhr67Yw3"
      },
      "outputs": [],
      "source": [
        "class GaussianMixture():\n",
        "    \"\"\"Gaussian Mixture model class separating data in clusters\n",
        "    \n",
        "    The KMeans class is used to initialize the probabilities p_ik of\n",
        "    each sample.\n",
        "    \n",
        "    Attributes:\n",
        "        n_components: An integer for the number of gaussian distributions in the mixture.\n",
        "        n_iter: A positive integer for the number of iterations to perform e-steps and m-steps.\n",
        "        priors: A torch tensor containing the probabilities of each class.\n",
        "        mu: A torch tensor for the mean of each gaussian.\n",
        "        sigma: A torch tensor for the covariance of each gaussian.\n",
        "        likelihoods: A torch tensor containing the likelihoods of each data in regards to each gaussian.\n",
        "        probs: A torch tensor holding the probabilities for each data to belong to each\n",
        "            gaussian.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_components, n_iter):\n",
        "        \"\"\"Inits the GaussianMixture class by setting n_components and n_iter\"\"\"\n",
        "        self.n_components = n_components\n",
        "        self.n_iter = n_iter\n",
        "        \n",
        "    def gaussian_likelihood(self, X, n):\n",
        "        \"\"\"Returns the gaussian likelihood of X from the nth gaussian\"\"\"\n",
        "        two_pi = torch.tensor(2 * math.pi, dtype=torch.float64)\n",
        "        fact = #TODO\n",
        "        \n",
        "        X_minus_mu = #TODO\n",
        "        sigma_inv = #TODO\n",
        "        return #TODO\n",
        "        \n",
        "    def e_step(self, X):\n",
        "        \"\"\"Assigns to each data the probability to belong to each gaussian\"\"\"\n",
        "        for j in range(self.n_components):\n",
        "            for i in range(self.n_samples):\n",
        "                self.likelihoods[i, j] = #TODO\n",
        "                \n",
        "        # Tensor to hold the probabilities that each data belongs to each gaussian\n",
        "        \n",
        "        prob_num = #TODO\n",
        "        prob_den = #TODO\n",
        "        self.probs = prob_num / prob_den\n",
        "        \n",
        "    def m_step(self, X):\n",
        "        \"\"\"Re-computes the parameters of each gaussian component\"\"\"\n",
        "        for j in range(self.n_components):\n",
        "            probs_j = self.probs[:, j]  # All probabilities from the j'th guassian\n",
        "            probs_j_sum = probs_j.sum()\n",
        "            self.priors[j] = probs_j.sum() / self.n_samples\n",
        "            \n",
        "            probs_j_uns = probs_j.unsqueeze(1)\n",
        "            \n",
        "            # Recomputes the means of each gaussian based on the probabilities of\n",
        "            # each data.\n",
        "            self.mu[j] = #TODO\n",
        "            \n",
        "            # Recomputes the covariance matrix of each data\n",
        "\n",
        "            #TODO\n",
        "            \n",
        "    def fit_predict(self, X):\n",
        "        \"\"\"Trains the models and returns the clusters\"\"\"\n",
        "        self.n_samples = X.shape[0]\n",
        "        self.n_features = X.shape[1]\n",
        "        \n",
        "        self.sigma = #TODO\n",
        "        self.mu = #TODO\n",
        "        self.priors = #TODO\n",
        "        \n",
        "        # Initialize the probabilities of each samples to belong\n",
        "        # to each gaussian by a KMeans\n",
        "\n",
        "        #TODO\n",
        "        \n",
        "        # Set the covariance parameter of each components to the\n",
        "        # covariance of the data\n",
        "        X_m = X - X.mean()\n",
        "        X_cov = X_m.T.mm(X_m)\n",
        "        for j in range(self.n_components):\n",
        "            self.sigma[j] = X_cov.clone()\n",
        "            # Select a random sample as the mean of each gaussian\n",
        "            self.mu[j] = #TODO\n",
        "        \n",
        "        self.likelihoods = torch.zeros((self.n_samples, self.n_components), dtype=torch.float64)\n",
        "        \n",
        "        self.m_step(X)  # Start with m_step to compute params of gaussians based on KMeans probabilities\n",
        "        for _ in range(self.n_iter):\n",
        "            self.e_step(X)\n",
        "            self.m_step(X)\n",
        "            \n",
        "        return self.probs.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**E- Declare a GMM model using \"GaussianMixture\" class with 3 clusters and 3 iterations and fit it with standard data and finally show clustered data in 2D figure**"
      ],
      "metadata": {
        "id": "7qwNrXFW5wdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAzB-I1r7Yw5"
      },
      "outputs": [],
      "source": [
        "#TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze-MepfU7Yw8"
      },
      "source": [
        "### Conclusion\n",
        "The KMeans algorithm can only describe spherical clusters while GMM can express elliptic clusters. In this experiment, the GMM undoubtably found the gaussian distributions that we generated. Whereas KMeans did a great job to separate the data but still lacks expressivity for clustering elliptic clusters.\n",
        "\n",
        "KMeans is significantly faster and simpler than other EM algorithm, but for data that has different sized and shaped clusters, GMM does a better job. However, GMM is known to be unstable. Indeed, the clusters can be a little bit different each time.\n",
        "\n",
        "If we had to choose one assertion to take out from this experiment, it would be that clustering methods are very powerful techniques suited for finding latent or missing variables."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Clustering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}